\chapter{Comparison and Evaluation}
\label{sec:comparison_and_evaluation}

In this section, we compare the fuzzy tuning technique with other tuning techniques present in AutoPas and evaluate its performance.

To measure the performance of the fuzzy tuning strategy, we also use the scenarios present in \texttt{md\_flexible} and compare the results with the other tuning strategies present in AutoPas. The benchmarks are run on the CoolMUC-2\footnote{\label{CoolMucSpecs}CoolMUC-2 is a supercomputer located at the Leibniz Supercomputing Centre in Garching, Germany. It consists of 812 Haswell-based nodes with 14 cores each. As a result of hyperthreading, each node supports up to 28 threads. More information can be found at \url{https://doku.lrz.de/coolmuc-2-11484376.html}} cluster and are repeated with 1, 12, 24, and 28 threads. We use the \texttt{timeSpentCalculatingForces} metric to evaluate the performance of the tuning strategies as it gives a good indication of the overall performance of the simulation.


\section{Exploding Liquid Benchmark (Included in Training Data)}
\label{sec:explodingLiquidBenchmark}

The exploding liquid benchmark simulates a high-density liquid that expands outwards as the simulation progresses. As the data of this scenario was included in the training data, we expect the fuzzy tuning technique to perform well. We only include the benchmark results with one thread for brevity, as the results for the other thread counts are very similar.

The plot in \autoref{fig:explodingTimings_1thread} shows the time spent calculating the forces for each tuning strategy throughout the simulation. The fuzzy tuning strategies typically perform close to optimal and are very stable. All other tuning strategies show a much higher variance caused by testing many configurations during the tuning phases.

The low tuning overhead is the most significant contributor to the performance of the fuzzy tuning strategies. As the tuning phases of the fuzzy tuning strategies are very short and mainly consist of evaluating already known suitable configurations, there is no overhead caused by the tuning phases. This contrasts with the classical tuning strategies, which spend significant time in the tuning phases.

To show this in more detail, we also include a boxplot of the time spent calculating the forces for each tuning strategy based on the current phase in \autoref{fig:explodingLiquidBoxplot_1thread}. All tuning strategies show similar timings during the simulation phases, as they eventually found a perfect configuration during the tuning phases but differ drastically in the tuning phases. The fuzzy tuning strategies have a much lower median time spent during tuning phases, with the individual tuning approach performing best. We see that the suitability approach performs worse than the other strategies during simulation phases because the suitability approach chose a suboptimal configuration for the first simulation phase, which was then corrected from the second tuning phase onwards. All other strategies eventually found a perfect configuration during the tuning phases, which caused them to perform better during the simulation phases.

This plot also shows that the interquartile range of the classical tuning strategies is very similar, with all having nearly identical means. However, all of them are plagued by massive outliers, sometimes taking ~ ten times longer than the median configuration and up to ~100 times longer than the optimal configuration. Those extremely bad configurations are the main reason for the poor performance of the classical tuning strategies.

The last plot in \autoref{fig:explodingLiquidTotalTime_1thread} shows the total time spent calculating the forces for each tuning strategy, again divided into simulation and tuning time. The fuzzy tuning strategies have the lowest total time, with practically no time spent in the tuning phases. Both fuzzy tuning approaches perform similarly and are by far the best-performing strategies, achieving a speedup of $\frac{t_{\text{FullSearch}}}{t_{\text{Fuzzy[Components]}}} \approx \frac{32.5s}{16.6s} \approx 1.96$ and $\frac{t_{\text{FullSearch}}}{t_{\text{Fuzzy[Suitability]}}} \approx \frac{32.5s}{20.3s} \approx 1.60$, respectively.

All other strategies typically spend more than 50\% of their time in tuning phases where they potentially encounter very bad configurations, which causes them to perform much worse than the fuzzy tuning strategies.


\section{Spinodal Decomposition Benchmark MPI (Related to Training Data)}

The spinodal decomposition benchmark simulates an unstable liquid that separates into two phases, each having different characteristics. To improve the performance of the simulation, we used four different MPI ranks, each running on 14 threads to simulate the scenario. As the complete spinodal decomposition benchmark was included in the training data, and the scenario is very homogeneous, we expect the fuzzy tuning strategies to also perform well in this scenario, even if there are no direct training data points for the division of the simulation into multiple MPI ranks.

For brevity, we only include the benchmark results for the 0th MPI rank, as the results for the other MPI ranks are nearly identical.

The plot in \autoref{fig:spinodalTimings_14thread} shows the time spent calculating the forces for each tuning strategy throughout the simulation. This time, we see a difference in both fuzzy tuning strategies, as the component tuning approach performs way better than the suitability approach for most of the simulation. By looking at the boxplots in \autoref{fig:spinodalBoxplot_14thread}, we see that the suitability approach has the lowest median time spent during the tuning phases. However, it struggles to find the optimal configurations, possibly because the used suitability threshold of 10\% is too low for this scenario. \autoref{sec:suitabilityThreshold} will investigate the effect of the suitability threshold on the performance of the simulation in more detail.

\autoref{fig:spinodal_14thread} shows that the component tuning approach again performs best, with a speedup of $\frac{t_{\text{FullSearch}}}{t_{\text{Fuzzy[Components]}}} \approx \frac{2236.1s}{1650.3s} \approx 1.35$. The suitability approach achieves a speedup of $\frac{t_{\text{FullSearch}}}{t_{\text{Fuzzy[Suitability]}}} \approx \frac{2236.1s}{1846.1s} \approx 1.21$. The suitability and predictive tuning approaches perform similarly and are in second place. Remarkably, the suitability approach performed reasonably well despite never finding the optimal configuration, mainly due to basically no time wasted during the tuning phases. This shows the importance of efficient tuning phases, as they can cause tremendous overhead if not done correctly.

\newpage



\begin{figure}[H]
    \centering

    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\columnwidth,trim={0cm 0cm 0cm 0.9cm},clip]{figures/Benchmark/ExplodingLiquid/timing_explodingLiquid_1.png}
        \caption{}
        \label{fig:explodingTimings_1thread}
    \end{subfigure}


    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\columnwidth,trim={0cm 0.5cm 0cm 1cm},clip]{figures/Benchmark/ExplodingLiquid/boxplot_explodingLiquid_1.png}
        \caption{}
        \label{fig:explodingLiquidBoxplot_1thread}
    \end{subfigure}

    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\columnwidth,trim={0cm 0.5cm 0cm 0.9cm},clip]{figures/Benchmark/ExplodingLiquid/total_time_explodingLiquid_1.png}
        \caption{}
        \label{fig:explodingLiquidTotalTime_1thread}
    \end{subfigure}


    \caption[Benchmark Results for the Exploding Liquid Scenario]{Exploding liquid benchmark with 1 thread. (a) Time spent calculating forces for every iteration. (b) Boxplots of time spent calculating forces divided into tuning- and simulation phases. (c) Total time spent calculating forces for tuning- and simulation phases. The Suitability approach uses a non-optimal threshold of 10\% (see \autoref{sec:suitabilityThreshold}).}
    \label{fig:explodingLiquid_1thread}
\end{figure}

\begin{figure}[H]
    \centering

    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\columnwidth,trim={0cm 0.2cm 0cm 0.9cm},clip]{figures/Benchmark/SpinodalDecompositionMPI/SpinodalDecompositionMPI_timings_SpinodalDecompositionMPI_14_0.png}
        \caption{}
        \label{fig:spinodalTimings_14thread}
    \end{subfigure}


    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\columnwidth,trim={0cm 0.5cm 0cm 1cm},clip]{figures/Benchmark/SpinodalDecompositionMPI/SpinodalDecompositionMPI_timings_boxplot_SpinodalDecompositionMPI_14_0.png}
        \caption{}
        \label{fig:spinodalBoxplot_14thread}
    \end{subfigure}

    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\columnwidth,trim={0cm 0.5cm 0cm 0.9cm},clip]{figures/Benchmark/SpinodalDecompositionMPI/SpinodalDecompositionMPI_timings_total_SpinodalDecompositionMPI_14_0.png}
        \caption{}
        \label{fig:spinodalTotalTime_14thread}
    \end{subfigure}


    \caption[Benchmark Results for the Spinodal Decomposition MPI Scenario]{0th Rank of the Spinodal decomposition benchmark (Total: 4 MPI ranks, 14 threads each). (a) Time spent calculating forces for every iteration. (b) Boxplots of time spent calculating forces divided into tuning- and simulation phases. (c) Total time spent calculating forces for tuning- and simulation phases. The suitability approach uses a non-optimal threshold of 10\% (see \autoref{sec:suitabilityThreshold}).}
    \label{fig:spinodal_14thread}
\end{figure}



\section{Further Analysis}

\subsection{Quality of Predictions During Tuning Phases}

As described above, a tremendous slowdown of the classical tuning strategies is caused by very bad configurations that are sometimes encountered during the tuning phases. To further illustrate this, we will investigate the speedup density distribution of all configurations evaluated during the tuning phases of the different strategies. In particular, we will look at the Exploding Liquid and Spinodal Decomposition MPI scenarios described above, as they represent \emph{small} and \emph{large} scenarios, respectively.

The plots in \autoref{fig:tuningPhaseSpeedup} show these relative speed distributions. All classical tuning strategies tend to encounter configurations with extremely low relative speed during the tuning phases. In the exploding-liquid benchmark, some bad configurations are $\sim10$ times slower than the winning configurations, while we observe iterations $\sim100$ times slower in the Spinodal Decomposition MPI scenario.

The fuzzy tuning strategies, especially the suitability approach, predict much better configurations during the tuning phases. Combined with the relatively small number of configurations evaluated during the tuning phases, this leads to very short tuning phases for this strategy while still finding close to optimal configurations, as shown in the previous sections. However, the relatively small number of evaluated configurations of the suitability approach is also a disadvantage, as it causes the strategy to sometimes miss the optimal configuration.
The component tuning approach evaluates more, possibly suboptimal, configurations during the tuning phases, which causes bigger spikes in the time spent calculating forces, which can be seen in \autoref{fig:explodingTimings_1thread} and \autoref{fig:spinodalTimings_14thread}. Those spikes are relatively short and rare and do not significantly impact the strategy's overall performance. This causes the component tuning approach to perform better than the suitability approach in most scenarios.
In \autoref{sec:suitabilityThreshold}, we will investigate the suitability threshold and its impact on the performance of the suitability approach.

\smallskip

A possible improvement to AutoPas would be to automatically detect such bad configurations while evaluating them and discard them early if they are significantly worse than the current best configuration. Such an improvement could drastically benefit every tuning strategy by significantly reducing the time spent in the tuning phases while still finding the same optimal configuration.


\newpage

\begin{figure}[H]
    \centering

    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\columnwidth,trim={1cm 0 0cm 1cm},clip]{figures/Benchmark/Observations/tuning_phase_speedup_explodingLiquid_1_zoomed.png}
        \caption{Exploding Liquid scenario with one thread.}
        \label{fig:explodingLiquidSpeedupDensity}
    \end{subfigure}


    \begin{subfigure}[c]{\textwidth}
        \includegraphics[width=\columnwidth,trim={1cm 0 0cm 1cm},clip]{figures/Benchmark/Observations/tuning_phase_speedup_SpinodalDecompositionMPI_14_0.png}
        \caption{Rank 0 of the Spinodal Decomposition MPI scenario with 14 threads.}
        \label{fig:spinodalSpeedupDensity}
    \end{subfigure}


    \caption[
        Relative speed distribution of configurations evaluated during tuning phases
    ]{The plot shows the relative speed distribution of all configurations evaluated during the tuning phases of both the Exploding Liquid and Spinodal Decomposition MPI scenarios calculated from the smoothed timings (see \autoref*{des:tuningdatafields}). The fuzzy tuning strategies generally encounter better configurations during the tuning phases, which improves their total performance.}
    \label{fig:tuningPhaseSpeedup}
\end{figure}

\subsection{Optimal Suitability Threshold}
\label{sec:suitabilityThreshold}

In previous measurements, the Component tuning approach performed better than the Suitability tuning approach, mainly due to the suitability approach not finding the optimal configuration during the tuning phases (see \autoref{fig:explodingLiquid_1thread} and \autoref{fig:spinodal_14thread}).
Currently, the rule file for the suitability approach specifies that only the top 10\% of configurations with the highest suitability should be selected, which may be too low, as this results in extremely few configurations being selected for the tuning phases, resulting in a high chance of not finding the optimal configuration.

To investigate this further, we ran the Exploding Liquid benchmark with different suitability thresholds, as shown in \autoref{fig:suitabilityThreshold}. Very low thresholds perform poorly, as they select too few configurations for the tuning phases, resulting in a high chance of not finding the optimal configuration. Very high thresholds also perform poorly, as high suitability values cause the strategy to behave like FullSeach as it selects nearly all configurations for the tuning phases. The optimal suitability threshold for this scenario is between 20\% and 40\%, which guarantees that the best configuration is selected for the tuning phases while still keeping the number of configurations low.


Therefore, we recommend using a slightly higher suitability threshold for the suitability approach between 20\% and 40\% to improve the chances of finding the optimal configuration during the tuning phases without causing too much overhead by evaluating too many configurations.


\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/Benchmark/SuitabilitySearch/SuitabilityExploding_timings_threshold_benchmark-cluster_suitability-tuning-exploding_1.png}
    \caption[
        Impact of the suitability threshold on the simulation performance
    ]{Exploding liquid benchmark with different suitability thresholds. The fastest runtimes are achieved with a threshold between 20\% and 40\%.}
    \label{fig:suitabilityThreshold}
\end{figure}

\newpage

\subsection{Generalization of Rule Extraction}

Previous measurements of the Exploding Liquid and the Spinodal Decomposition MPI benchmark showed that the fuzzy tuning strategies perform well. However, those scenarios were included in the training data, which could have biased the results in favor of the fuzzy tuning strategies. To investigate the generalization of the rule extraction process, we reran the rule extraction process without the Exploding Liquid scenario in the training data and performed the Exploding Liquid benchmark again.

The results in \autoref{fig:explodingTimings_1thread_noTrainingData} show that the fuzzy tuning strategies still perform remarkably well, even without ever encountering the Exploding Liquid scenario during the rule extraction process. Therefore, we conclude that the rule extraction process is robust and can be generalized to similar scenarios, even if they were not included in the training data.

\begin{figure}[H]
    \centering

    \includegraphics[width=\columnwidth,trim={0cm 0 0cm 0.9cm},clip]{figures/Benchmark/ExplodingLiquidHoldout/total_time_explodingLiquid_1.png}
    \caption[
        Benchmark Results for the Exploding Liquid Scenario (Holdout)
    ]{
        Total time spent calculating forces for tuning- and simulation phases for the Exploding Liquid benchmark without the scenario being included in the training data. The fuzzy tuning strategies still perform best, with comparable performance to the previous measurements.
    }
    \label{fig:explodingTimings_1thread_noTrainingData}
\end{figure}