@article{CROCKETT20062809,
  title    = {On constructing a fuzzy inference framework using crisp decision trees},
  journal  = {Fuzzy Sets and Systems},
  volume   = {157},
  number   = {21},
  pages    = {2809-2832},
  year     = {2006},
  issn     = {0165-0114},
  doi      = {https://doi.org/10.1016/j.fss.2006.06.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S0165011406002533},
  author   = {Keeley Crockett and Zuhair Bandar and David Mclean and James O’Shea},
  keywords = {Fuzzy inference systems, Decision trees, Genetic algorithms},
  abstract = {This paper proposes a framework which consists of a novel fuzzy inference algorithm to generate fuzzy decision trees from induced crisp decision trees. Fuzzy theoretical techniques are used to fuzzify crisp decision trees in order to soften sharp decision boundaries at decision nodes inherent in this type of trees. A framework for the investigation of various types of membership functions and fuzzy inference techniques is proposed. Once the decision tree has been fuzzified, all branches throughout the tree will fire, resulting in a membership grade being generated at each branch. Five different fuzzy inference mechanisms are used to investigate the degree of interaction between membership grades on each path in the decision tree, which ultimately leads to a final crisp classification. A genetic algorithm is used to optimize and automatically determine the set of fuzzy regions for all branches and simultaneously the degree in which the inference parameters will be applied. Comparisons between crisp trees and the fuzzified trees suggest that the later fuzzy tree is significantly more robust and produces a more balanced classification. In addition, the results obtained from five real-world data sets show that there is a significant improvement in the accuracy of the fuzzy trees when compared with crisp trees.}
}

@book{10.5555/2380985,
  author    = {Murphy, Kevin P.},
  title     = {Machine Learning: A Probabilistic Perspective},
  year      = {2012},
  isbn      = {0262018020},
  publisher = {The MIT Press},
  abstract  = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.}
}
@article{GRATL2022108262,
  author   = {Gratl, Fabio Alexander and Seckler, Steffen and Bungartz, Hans-Joachim and Neumann, Philipp},
  title    = {N Ways to Simulate Short-Range Particle Systems: Automated Algorithm Selection with the Node-Level Library AutoPas},
  journal  = {Computer Physics Communications},
  year     = {2021},
  volume   = {273},
  month    = {},
  number   = {},
  pages    = {108262},
  issn     = {},
  doi      = {10.1016/j.cpc.2021.108262},
  language = {en},
  abstract = {AutoPas is an open-source C++ library delivering optimal node-level performance by providing the ideal algorithmic configuration for an arbitrary scenario in a given short-range particle simulation. It acts as a black-box container, internally implementing an extensive set of algorithms, parallelization strategies, and optimizations that are combined dynamically according to the state of the simulation via auto-tuning. This paper gives an overview of the high-level user perspective, as well as the internal view, covering the implemented techniques and features. The library is showcased by incorporating it into the codes LAMMPS and ls1 mardyn, and by investigating various applications. We further outline node-level shared-memory performance and scalability of our auto-tuning software which is comparable to LAMMPS.},
  keywords = {HPC; N-Body Simulation; Molecular Dynamics; Auto-Tuning; Automatic Algorithm Selection; Dynamic Tuning},
  note     = {},
  url      = {https://www.researchgate.net/publication/357143093_N_Ways_to_Simulate_Short-Range_Particle_Systems_Automated_Algorithm_Selection_with_the_Node-Level_Library_AutoPas}
}

@article{VICCIONE2008625,
  author   = {Viccione, G. and Bovolin, V. and Carratelli, E. Pugliese},
  title    = {Defining and optimizing algorithms for neighbouring particle identification in SPH fluid simulations},
  journal  = {International Journal for Numerical Methods in Fluids},
  volume   = {58},
  number   = {6},
  pages    = {625-638},
  keywords = {SPH, Lagrangian methods, Verlet list, cell-linked list},
  doi      = {https://doi.org/10.1002/fld.1761},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/fld.1761},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/fld.1761},
  abstract = {Abstract Lagrangian particle methods such as smoothed particle hydrodynamics (SPH) are very demanding in terms of computing time for large domains. Since the numerical integration of the governing equations is only carried out for each particle on a restricted number of neighbouring ones located inside a cut-off radius rc, a substantial part of the computational burden depends on the actual search procedure; it is therefore vital that efficient methods are adopted for such a search. The cut-off radius is indeed much lower than the typical domain's size; hence, the number of neighbouring particles is only a little fraction of the total number. Straightforward determination of which particles are inside the interaction range requires the computation of all pair-wise distances, a procedure whose computational time would be unpractical or totally impossible for large problems. Two main strategies have been developed in the past in order to reduce the unnecessary computation of distances: the first based on dynamically storing each particle's neighbourhood list (Verlet list) and the second based on a framework of fixed cells. The paper presents the results of a numerical sensitivity study on the efficiency of the two procedures as a function of such parameters as the Verlet size and the cell dimensions. An insight is given into the relative computational burden; a discussion of the relative merits of the different approaches is also given and some suggestions are provided on the computational and data structure of the neighbourhood search part of SPH codes. Copyright © 2008 John Wiley \& Sons, Ltd.},
  year     = {2008}
}

@inproceedings{GRATL2019748,
  author    = {Gratl, Fabio Alexander and Seckler, Steffen and Tchipev, Nikola and Bungartz, Hans-Joachim and Neumann, Philipp},
  booktitle = {2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  title     = {AutoPas: Auto-Tuning for Particle Simulations},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {748-757},
  keywords  = {Force;Heuristic algorithms;Computational modeling;Dynamics;Optimization;Software algorithms;Adaptation models;HPC;Molecular Dynamics;Auto-Tuning;Automatic Algorithm Selection;Dynamic Tuning},
  doi       = {10.1109/IPDPSW.2019.00125}
}